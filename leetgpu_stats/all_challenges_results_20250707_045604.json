{
  "collection_timestamp": "2025-07-07T04:56:04.394464",
  "total_challenges": 36,
  "total_combinations": 900,
  "challenges": [
    {
      "name": "vector-addition",
      "url": "https://leetgpu.com/challenges/vector-addition",
      "title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000"
    },
    {
      "name": "matrix-multiplication",
      "url": "https://leetgpu.com/challenges/matrix-multiplication",
      "title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096"
    },
    {
      "name": "matrix-transpose",
      "url": "https://leetgpu.com/challenges/matrix-transpose",
      "title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows"
    },
    {
      "name": "color-inversion",
      "url": "https://leetgpu.com/challenges/color-inversion",
      "title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608."
    },
    {
      "name": "1d-convolution",
      "url": "https://leetgpu.com/challenges/1d-convolution",
      "title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size"
    },
    {
      "name": "reverse-array",
      "url": "https://leetgpu.com/challenges/reverse-array",
      "title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000"
    },
    {
      "name": "relu-activation",
      "url": "https://leetgpu.com/challenges/relu-activation",
      "title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000"
    },
    {
      "name": "leaky-relu",
      "url": "https://leetgpu.com/challenges/leaky-relu",
      "title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0"
    },
    {
      "name": "rainbow-table",
      "url": "https://leetgpu.com/challenges/rainbow-table",
      "title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647"
    },
    {
      "name": "matrix-copy",
      "url": "https://leetgpu.com/challenges/matrix-copy",
      "title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers"
    },
    {
      "name": "monte-carlo-integration",
      "url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations."
    },
    {
      "name": "reduction",
      "url": "https://leetgpu.com/challenges/reduction",
      "title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float"
    },
    {
      "name": "softmax",
      "url": "https://leetgpu.com/challenges/softmax",
      "title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000"
    },
    {
      "name": "softmax-attention",
      "url": "https://leetgpu.com/challenges/softmax-attention",
      "title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024"
    },
    {
      "name": "2d-convolution",
      "url": "https://leetgpu.com/challenges/2d-convolution",
      "title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols"
    },
    {
      "name": "histogramming",
      "url": "https://leetgpu.com/challenges/histogramming",
      "title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024"
    },
    {
      "name": "sorting",
      "url": "https://leetgpu.com/challenges/sorting",
      "title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000"
    },
    {
      "name": "prefix-sum",
      "url": "https://leetgpu.com/challenges/prefix-sum",
      "title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float"
    },
    {
      "name": "dot-product",
      "url": "https://leetgpu.com/challenges/dot-product",
      "title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000"
    },
    {
      "name": "sparse-matrix-vector-multiplication",
      "url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)"
    },
    {
      "name": "gemm-fp16",
      "url": "https://leetgpu.com/challenges/gemm-fp16",
      "title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096"
    },
    {
      "name": "categorical-cross-entropy-loss",
      "url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C"
    },
    {
      "name": "password-cracking-fnv-1a",
      "url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet"
    },
    {
      "name": "mean-squared-error",
      "url": "https://leetgpu.com/challenges/mean-squared-error",
      "title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0"
    },
    {
      "name": "gaussian-blur",
      "url": "https://leetgpu.com/challenges/gaussian-blur",
      "title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)"
    },
    {
      "name": "top-k-selection",
      "url": "https://leetgpu.com/challenges/top-k-selection",
      "title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats"
    },
    {
      "name": "batched-matrix-multiplication-fp32",
      "url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024"
    },
    {
      "name": "quantized-matrix-multiplication-int8",
      "url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127"
    },
    {
      "name": "ordinary-least-squares-regression",
      "url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2"
    },
    {
      "name": "logistic-regression",
      "url": "https://leetgpu.com/challenges/logistic-regression",
      "title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2"
    },
    {
      "name": "radix-sort",
      "url": "https://leetgpu.com/challenges/radix-sort",
      "title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)"
    },
    {
      "name": "matrix-power",
      "url": "https://leetgpu.com/challenges/matrix-power",
      "title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)"
    },
    {
      "name": "3d-convolution",
      "url": "https://leetgpu.com/challenges/3d-convolution",
      "title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols"
    },
    {
      "name": "multi-head-self-attention",
      "url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0"
    },
    {
      "name": "swarm-intelligence-flocking-simulation",
      "url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats"
    },
    {
      "name": "k-means-clustering",
      "url": "https://leetgpu.com/challenges/k-means-clustering",
      "title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k"
    }
  ],
  "results": [
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.002 ms",
      "fastest_ms": 0.002,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:24:23.484291"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1757 ms",
      "fastest_ms": 0.1757,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:24:36.933674"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0998 ms",
      "fastest_ms": 0.0998,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:24:50.383362"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0774 ms",
      "fastest_ms": 0.0774,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:25:03.829071"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.048 ms",
      "fastest_ms": 0.048,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:25:17.282589"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.182 ms",
      "fastest_ms": 1.182,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:25:30.721302"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2009 ms",
      "fastest_ms": 0.2009,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:25:44.160562"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1214 ms",
      "fastest_ms": 0.1214,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:25:57.597535"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0963 ms",
      "fastest_ms": 0.0963,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:26:11.042986"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0705 ms",
      "fastest_ms": 0.0705,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:26:24.471730"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.2038 ms",
      "fastest_ms": 1.2038,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:26:37.896864"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1768 ms",
      "fastest_ms": 0.1768,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:26:51.341012"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1034 ms",
      "fastest_ms": 0.1034,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:27:04.787263"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.078 ms",
      "fastest_ms": 0.078,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:27:18.208741"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0488 ms",
      "fastest_ms": 0.0488,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:27:31.638085"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.2245 ms",
      "fastest_ms": 1.2245,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:27:45.079465"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.226 ms",
      "fastest_ms": 0.226,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:27:58.519049"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.162 ms",
      "fastest_ms": 0.162,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:28:11.962302"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:28:25.399430"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1059 ms",
      "fastest_ms": 0.1059,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:28:38.843870"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.213 ms",
      "fastest_ms": 2.213,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:28:52.286566"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "1.2522 ms",
      "fastest_ms": 1.2522,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:29:05.710472"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.8807 ms",
      "fastest_ms": 0.8807,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:29:19.150476"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:29:32.586225"
    },
    {
      "challenge_name": "vector-addition",
      "challenge_title": "Easy\nVector Addition\nImplement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. The program should take two input vectors of equal length and produce a single output vector containing their sum. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector C Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: C = [6.0, 8.0, 10.0, 12.0] Example 2: Input: A = [1.5, 1.5, 1.5] B = [2.3, 2.3, 2.3] Output: C = [3.8, 3.8, 3.8] Constraints Input vectors A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/vector-addition",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:29:46.015290"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "95.2414 ms",
      "fastest_ms": 95.2414,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:30:17.743339"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "27.1809 ms",
      "fastest_ms": 27.1809,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:30:31.201338"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "12.8991 ms",
      "fastest_ms": 12.8991,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:30:44.680707"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "23.9402 ms",
      "fastest_ms": 23.9402,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:30:58.133852"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "12.4612 ms",
      "fastest_ms": 12.4612,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:31:11.583731"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "116.7219 ms",
      "fastest_ms": 116.7219,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:31:25.037303"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "38.929 ms",
      "fastest_ms": 38.929,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:31:38.501582"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "13.1906 ms",
      "fastest_ms": 13.1906,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:31:51.971712"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": "67.249 ms",
      "fastest_ms": 67.249,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:32:05.409044"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "11.4079 ms",
      "fastest_ms": 11.4079,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:32:18.859785"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "92.5438 ms",
      "fastest_ms": 92.5438,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:32:32.325092"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "21.6621 ms",
      "fastest_ms": 21.6621,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:32:45.761162"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": "7.2119 ms",
      "fastest_ms": 7.2119,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:32:59.214337"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": "8.1859 ms",
      "fastest_ms": 8.1859,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:33:12.654721"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "6.2375 ms",
      "fastest_ms": 6.2375,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:33:26.116983"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "584.1008 ms",
      "fastest_ms": 584.1008,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:33:39.596140"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:33:53.033316"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": "91.2169 ms",
      "fastest_ms": 91.2169,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:34:06.482331"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:34:19.927596"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:34:33.370195"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "599.641 ms",
      "fastest_ms": 599.641,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:34:46.821992"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "154.0141 ms",
      "fastest_ms": 154.0141,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:35:00.248239"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "87.6629 ms",
      "fastest_ms": 87.6629,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:35:13.704386"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:35:27.148618"
    },
    {
      "challenge_name": "matrix-multiplication",
      "challenge_title": "Easy\nMatrix Multiplication\nWrite a program that multiplies two matrices of 32-bit floating point numbers on a GPU. Given matrix \\(A\\) of dimensions \\(M \\times N\\) and matrix \\(B\\) of dimensions \\(N \\times K\\), compute the product matrix \\(C = A \\times B\\), which will have dimensions \\(M \\times K\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in matrix C Example 1: Input: Matrix \\(A\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 5.0 & 6.0 \\\\ 7.0 & 8.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(2 \\times 2\\)): \\[ \\begin{bmatrix} 19.0 & 22.0 \\\\ 43.0 & 50.0 \\end{bmatrix} \\] Example 2: Input: Matrix \\(A\\) (\\(1 \\times 3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(3 \\times 1\\)): \\[ \\begin{bmatrix} 4.0 \\\\ 5.0 \\\\ 6.0 \\end{bmatrix} \\] Output: Matrix \\(C\\) (\\(1 \\times 1\\)): \\[ \\begin{bmatrix} 32.0 \\end{bmatrix} \\] Constraints 1 &le; M, N, K &le; 8192 Performance is measured with M = 8192, N = 6144, K = 4096",
      "challenge_url": "https://leetgpu.com/challenges/matrix-multiplication",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:35:40.595554"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.8084 ms",
      "fastest_ms": 1.8084,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:36:12.285229"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2222 ms",
      "fastest_ms": 0.2222,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:36:25.755655"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1251 ms",
      "fastest_ms": 0.1251,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:36:39.208791"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.1027 ms",
      "fastest_ms": 0.1027,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:36:52.654699"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0782 ms",
      "fastest_ms": 0.0782,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:37:06.095352"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.6983 ms",
      "fastest_ms": 1.6983,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:37:19.539889"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2392 ms",
      "fastest_ms": 0.2392,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:37:32.971331"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1473 ms",
      "fastest_ms": 0.1473,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:37:46.400696"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:37:59.823369"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:38:13.276181"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0146 ms",
      "fastest_ms": 0.0146,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:38:26.708517"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.5174 ms",
      "fastest_ms": 0.5174,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:38:40.134610"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.3623 ms",
      "fastest_ms": 0.3623,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:38:53.557614"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:39:06.987504"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1924 ms",
      "fastest_ms": 0.1924,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:39:20.430541"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.0684 ms",
      "fastest_ms": 2.0684,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:39:33.881515"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:39:47.314456"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.5024 ms",
      "fastest_ms": 0.5024,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:40:00.737709"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:40:14.170509"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:40:27.609620"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "61.608 ms",
      "fastest_ms": 61.608,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:40:41.062273"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "16.3754 ms",
      "fastest_ms": 16.3754,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:40:54.506059"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "16.8374 ms",
      "fastest_ms": 16.8374,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:41:07.941452"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:41:21.389234"
    },
    {
      "challenge_name": "matrix-transpose",
      "challenge_title": "Easy\nMatrix Transpose\nWrite a program that transposes a matrix of 32-bit floating point numbers on a GPU. The transpose of a matrix switches its rows and columns. Given a matrix \\(A\\) of dimensions \\(rows \\times cols\\), the transpose \\(A^T\\) will have dimensions \\(cols \\times rows\\). All matrices are stored in row-major format. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the matrix output Example 1: Input: 2\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Output: 3\u00d72 matrix \\[ \\begin{bmatrix} 1.0 & 4.0 \\\\ 2.0 & 5.0 \\\\ 3.0 & 6.0 \\end{bmatrix} \\] Example 2: Input: 3\u00d71 matrix \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix} \\] Output: 1\u00d73 matrix \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 rows, cols \u2264 8192 Input matrix dimensions: rows \u00d7 cols Output matrix dimensions: cols \u00d7 rows",
      "challenge_url": "https://leetgpu.com/challenges/matrix-transpose",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:41:34.812793"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.6781 ms",
      "fastest_ms": 0.6781,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:42:06.477750"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0969 ms",
      "fastest_ms": 0.0969,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:42:19.909123"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0595 ms",
      "fastest_ms": 0.0595,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:42:33.337376"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0496 ms",
      "fastest_ms": 0.0496,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:42:46.753682"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.032 ms",
      "fastest_ms": 0.032,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:43:00.189165"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7105 ms",
      "fastest_ms": 0.7105,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:43:13.631723"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1368 ms",
      "fastest_ms": 0.1368,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:43:27.047822"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:43:40.460360"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:43:53.923086"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0706 ms",
      "fastest_ms": 0.0706,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:44:07.332726"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.4224 ms",
      "fastest_ms": 1.4224,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:44:20.756624"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:44:34.173731"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:44:47.588023"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:45:01.020277"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1463 ms",
      "fastest_ms": 0.1463,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:45:14.437746"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.924 ms",
      "fastest_ms": 0.924,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:45:27.866917"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:45:41.304413"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:45:54.743607"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:46:08.159176"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:46:21.583309"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:46:35.009099"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:46:48.438294"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:47:01.850680"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:47:15.272572"
    },
    {
      "challenge_name": "color-inversion",
      "challenge_title": "Easy\nColor Inversion\nWrite a program to invert the colors of an image. The image is represented as a 1D array of RGBA (Red, Green, Blue, Alpha) values, where each component is an 8-bit unsigned integer (unsigned char). Color inversion is performed by subtracting each color component (R, G, B) from 255. The Alpha component should remain unchanged. The input array image will contain width * height * 4 elements. The first 4 elements represent the RGBA values of the top-left pixel, the next 4 elements represent the pixel to its right, and so on. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array image Example 1: Input: image = [255, 0, 128, 255, 0, 255, 0, 255], width=1, height=2 Output: [0, 255, 127, 255, 255, 0, 255, 255] Example 2: Input: image = [10, 20, 30, 255, 100, 150, 200, 255], width=2, height=1 Output: [245, 235, 225, 255, 155, 105, 55, 255] Constraints 1 &le; width &le; 4096 1 &le; height &le; 4096 width * height &le; 8,388,608.",
      "challenge_url": "https://leetgpu.com/challenges/color-inversion",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:47:28.689166"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.0055 ms",
      "fastest_ms": 2.0055,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:48:00.396174"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.3941 ms",
      "fastest_ms": 0.3941,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:48:13.838841"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1833 ms",
      "fastest_ms": 0.1833,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:48:27.282042"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.1839 ms",
      "fastest_ms": 0.1839,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:48:40.745256"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1679 ms",
      "fastest_ms": 0.1679,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:48:54.198905"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "6.0309 ms",
      "fastest_ms": 6.0309,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:49:07.653851"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "1.7363 ms",
      "fastest_ms": 1.7363,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:49:21.090678"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "1.2085 ms",
      "fastest_ms": 1.2085,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:49:34.533830"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.561 ms",
      "fastest_ms": 0.561,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:49:47.976746"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "1.0478 ms",
      "fastest_ms": 1.0478,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:50:01.429069"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "60.9001 ms",
      "fastest_ms": 60.9001,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:50:14.869722"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:50:28.283389"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:50:41.714019"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:50:55.131371"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:51:08.558495"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "7.36 ms",
      "fastest_ms": 7.36,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:51:21.992543"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:51:35.427665"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:51:48.861609"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:52:02.319144"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:52:15.762208"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "44.3167 ms",
      "fastest_ms": 44.3167,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:52:29.224757"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "12.0893 ms",
      "fastest_ms": 12.0893,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:52:42.664525"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "7.3935 ms",
      "fastest_ms": 7.3935,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:52:56.114465"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:53:09.570226"
    },
    {
      "challenge_name": "1d-convolution",
      "challenge_title": "Easy\n1D Convolution\nImplement a program that performs a 1D convolution operation. Given an input array and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of two arrays: input: A 1D array of 32-bit floating-point numbers. kernel: A 1D array of 32-bit floating-point numbers representing the convolution kernel. The output should be written to the output array, which will have a size of input_size - kernel_size + 1. The convolution operation is defined mathematically as: \\[ output[i] = \\sum_{j=0}^{kernel\\_size-1} input[i + j] \\cdot kernel[j] \\] where \\(i\\) ranges from 0 to \\(input\\_size - kernel\\_size\\). Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input = [1, 2, 3, 4, 5], kernel = [1, 0, -1] Output: [-2, -2, -2] Example 2: Input: input = [2, 4, 6, 8], kernel = [0.5, 0.2] Output: [1.8, 3.2, 4.6] Constraints 1 &le; input_size &le; 1,000,000 1 &le; kernel_size &le; 2047 kernel_size &le; input_size",
      "challenge_url": "https://leetgpu.com/challenges/1d-convolution",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:53:23.001959"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7948 ms",
      "fastest_ms": 0.7948,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:53:54.675193"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1189 ms",
      "fastest_ms": 0.1189,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:54:08.112470"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0746 ms",
      "fastest_ms": 0.0746,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:54:21.539738"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0593 ms",
      "fastest_ms": 0.0593,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:54:34.969897"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0383 ms",
      "fastest_ms": 0.0383,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:54:48.408014"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.8365 ms",
      "fastest_ms": 0.8365,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:55:01.849594"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1615 ms",
      "fastest_ms": 0.1615,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:55:15.285746"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:55:28.709291"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.2817 ms",
      "fastest_ms": 0.2817,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:55:42.144017"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.2365 ms",
      "fastest_ms": 0.2365,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:55:55.579814"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.8538 ms",
      "fastest_ms": 0.8538,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:56:09.018881"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:56:22.431428"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:56:35.871737"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:56:49.307478"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:57:02.727107"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.0857 ms",
      "fastest_ms": 1.0857,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:57:16.162373"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:57:29.599650"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:57:43.026969"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:57:56.446689"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:58:09.881212"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "4.1154 ms",
      "fastest_ms": 4.1154,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:58:23.318234"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "1.8224 ms",
      "fastest_ms": 1.8224,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:58:36.745698"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "1.72 ms",
      "fastest_ms": 1.72,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:58:50.180365"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:59:03.612264"
    },
    {
      "challenge_name": "reverse-array",
      "challenge_title": "Easy\nReverse Array\nImplement a program that reverses an array of 32-bit floating point numbers in-place. The program should perform an in-place reversal of input. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored back in input Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [4.0, 3.0, 2.0, 1.0] Example 2: Input: [1.5, 2.5, 3.5] Output: [3.5, 2.5, 1.5] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/reverse-array",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:59:17.039557"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7956 ms",
      "fastest_ms": 0.7956,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T01:59:48.680334"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1257 ms",
      "fastest_ms": 0.1257,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:00:02.123170"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0747 ms",
      "fastest_ms": 0.0747,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:00:15.571090"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0595 ms",
      "fastest_ms": 0.0595,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:00:29.021329"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0417 ms",
      "fastest_ms": 0.0417,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:00:42.460533"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.828 ms",
      "fastest_ms": 0.828,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:00:55.875265"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1546 ms",
      "fastest_ms": 0.1546,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:01:09.319039"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1278 ms",
      "fastest_ms": 0.1278,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:01:22.740845"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:01:36.184165"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.054 ms",
      "fastest_ms": 0.054,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:01:49.608639"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.8344 ms",
      "fastest_ms": 0.8344,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:02:03.041082"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:02:16.472311"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:02:29.904307"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:02:43.340714"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1435 ms",
      "fastest_ms": 0.1435,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:02:56.776870"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.9275 ms",
      "fastest_ms": 0.9275,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:03:10.207993"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:03:23.640969"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:03:37.082651"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:03:50.523864"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:04:03.953220"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.9412 ms",
      "fastest_ms": 2.9412,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:04:17.393461"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "1.8716 ms",
      "fastest_ms": 1.8716,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:04:30.846283"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "1.2559 ms",
      "fastest_ms": 1.2559,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:04:44.269545"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:04:57.710188"
    },
    {
      "challenge_name": "relu-activation",
      "challenge_title": "Easy\nReLU Activation\nImplement a program that performs the Rectified Linear Unit (ReLU) activation function on a vector of 32-bit floating point numbers. The ReLU function sets all negative values to zero and leaves positive values unchanged: \\[\\text{ReLU}(x) = \\max(0, x)\\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in output Example 1: Input: input = [-2.0, -1.0, 0.0, 1.0, 2.0] Output: output = [0.0, 0.0, 0.0, 1.0, 2.0] Example 2: Input: input = [-3.5, 0.0, 4.2] Output: output = [0.0, 0.0, 4.2] Constraints 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/relu-activation",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:05:11.152412"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.5842 ms",
      "fastest_ms": 1.5842,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:05:42.866964"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2337 ms",
      "fastest_ms": 0.2337,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:05:56.332513"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1413 ms",
      "fastest_ms": 0.1413,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:06:09.773587"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.102 ms",
      "fastest_ms": 0.102,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:06:23.208351"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0677 ms",
      "fastest_ms": 0.0677,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:06:36.667774"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.6197 ms",
      "fastest_ms": 1.6197,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:06:50.106196"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2574 ms",
      "fastest_ms": 0.2574,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:07:03.541982"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1574 ms",
      "fastest_ms": 0.1574,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:07:16.976807"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:07:30.420106"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0837 ms",
      "fastest_ms": 0.0837,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:07:43.862791"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "3.2993 ms",
      "fastest_ms": 3.2993,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:07:57.319866"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:08:10.745451"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:08:24.174269"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:08:37.591598"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.273 ms",
      "fastest_ms": 0.273,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:08:51.014518"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.7665 ms",
      "fastest_ms": 1.7665,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:09:04.452581"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:09:17.864851"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:09:31.302224"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:09:44.738800"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:09:58.169462"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "3.6027 ms",
      "fastest_ms": 3.6027,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:10:11.615643"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "1.8405 ms",
      "fastest_ms": 1.8405,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:10:25.048566"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "1.835 ms",
      "fastest_ms": 1.835,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:10:38.476557"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:10:51.890970"
    },
    {
      "challenge_name": "leaky-relu",
      "challenge_title": "Easy\nLeaky ReLU\nImplement a program that performs the leaky ReLU activation function on a vector of floating-point numbers. The leaky ReLU function is defined as: \\[ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\] where \\(\\alpha\\) is a small positive constant (0.01 in this problem). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in vector output Use \\(\\alpha = 0.01\\) as the leaky coefficient Example 1: Input: x = [1.0, -2.0, 3.0, -4.0] Output: y = [1.0, -0.02, 3.0, -0.04] Example 2: Input: x = [-1.5, 0.0, 2.5, -3.0] Output: y = [-0.015, 0.0, 2.5, -0.03] Constraints 1 \u2264 N \u2264 100,000,000 -1000.0 \u2264 input[i] \u2264 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/leaky-relu",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:11:05.317566"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.1943 ms",
      "fastest_ms": 0.1943,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:11:37.032109"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0576 ms",
      "fastest_ms": 0.0576,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:11:50.483536"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0364 ms",
      "fastest_ms": 0.0364,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:12:03.962847"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0397 ms",
      "fastest_ms": 0.0397,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:12:17.457731"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.037 ms",
      "fastest_ms": 0.037,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:12:30.929027"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.2684 ms",
      "fastest_ms": 0.2684,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:12:44.402515"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0831 ms",
      "fastest_ms": 0.0831,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:12:57.901992"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0629 ms",
      "fastest_ms": 0.0629,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:13:11.371344"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:13:24.848126"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0568 ms",
      "fastest_ms": 0.0568,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:13:38.315150"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "84.1218 ms",
      "fastest_ms": 84.1218,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:13:51.783398"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "11.6165 ms",
      "fastest_ms": 11.6165,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:14:05.242783"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": "7.5803 ms",
      "fastest_ms": 7.5803,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:14:18.698725"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:14:32.161176"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:14:45.619900"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:14:59.092100"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:15:12.558444"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:15:26.021098"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:15:39.490208"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:15:52.962849"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "72.2697 ms",
      "fastest_ms": 72.2697,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:16:06.418123"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "57.4002 ms",
      "fastest_ms": 57.4002,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:16:19.878650"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "41.9664 ms",
      "fastest_ms": 41.9664,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:16:33.358213"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:16:46.826720"
    },
    {
      "challenge_name": "rainbow-table",
      "challenge_title": "Easy\nRainbow Table\nImplement a program that performs R rounds of parallel hashing on an array of 32-bit integers using the provided hash function. The hash should be applied R times iteratively (the output of one round becomes the input to the next). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in array output Example 1: Input: numbers = [123, 456, 789], R = 2 Output: hashes = [1636807824, 1273011621, 2193987222] Example 2: Input: numbers = [0, 1, 2147483647], R = 3 Output: hashes = [96754810, 3571711400, 2006156166] Constraints 1 \u2264 N \u2264 10,000,000 1 \u2264 R \u2264 100 0 \u2264 input[i] \u2264 2147483647",
      "challenge_url": "https://leetgpu.com/challenges/rainbow-table",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:17:00.318271"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.5366 ms",
      "fastest_ms": 0.5366,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:17:32.004950"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0781 ms",
      "fastest_ms": 0.0781,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:17:45.451360"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0492 ms",
      "fastest_ms": 0.0492,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:17:58.894239"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0416 ms",
      "fastest_ms": 0.0416,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:18:12.342165"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0278 ms",
      "fastest_ms": 0.0278,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:18:25.793620"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.6343 ms",
      "fastest_ms": 0.6343,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:18:39.249015"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.4666 ms",
      "fastest_ms": 0.4666,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:18:52.677830"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:19:06.095373"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:19:19.525770"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0467 ms",
      "fastest_ms": 0.0467,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:19:32.956579"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.5642 ms",
      "fastest_ms": 0.5642,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:19:46.402768"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:19:59.832548"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:20:13.264595"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:20:26.702903"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0343 ms",
      "fastest_ms": 0.0343,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:20:40.135000"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7357 ms",
      "fastest_ms": 0.7357,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:20:53.570831"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:21:07.008256"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:21:20.442592"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:21:33.877267"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:21:47.312410"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.7577 ms",
      "fastest_ms": 1.7577,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:22:00.755527"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.9381 ms",
      "fastest_ms": 0.9381,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:22:14.194719"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.8819 ms",
      "fastest_ms": 0.8819,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:22:27.635161"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:22:41.074321"
    },
    {
      "challenge_name": "matrix-copy",
      "challenge_title": "Easy\nMatrix Copy\nImplement a program that copies an \\(N \\times N\\) matrix of 32-bit floating point numbers from input array \\(A\\) to output array \\(B\\) on the GPU. The program should perform a direct element-wise copy so that \\(B_{i,j} = A_{i,j}\\) for all valid indices. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in matrix B Example 1: Input: A = [[1.0, 2.0], [3.0, 4.0]] Output: B = [[1.0, 2.0], [3.0, 4.0]] Example 2: Input: A = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Output: B = [[5.5, 6.6, 7.7], [8.8, 9.9, 10.1], [11.2, 12.3, 13.4]] Constraints 1 &le; N &le; 4096 All elements are 32-bit floating point numbers",
      "challenge_url": "https://leetgpu.com/challenges/matrix-copy",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:22:54.517778"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0289 ms",
      "fastest_ms": 0.0289,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:23:26.230795"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0139 ms",
      "fastest_ms": 0.0139,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:23:39.686907"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0126 ms",
      "fastest_ms": 0.0126,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:23:53.109528"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0143 ms",
      "fastest_ms": 0.0143,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:24:06.550773"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0143 ms",
      "fastest_ms": 0.0143,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:24:19.967732"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0746 ms",
      "fastest_ms": 0.0746,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:24:33.396559"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:24:46.813341"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:25:00.226775"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:25:13.647032"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "1.7539 ms",
      "fastest_ms": 1.7539,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:25:27.054904"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0708 ms",
      "fastest_ms": 0.0708,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:25:40.465290"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:25:53.857480"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:26:07.250072"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:26:20.652147"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:26:34.055951"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.4034 ms",
      "fastest_ms": 0.4034,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:26:47.461618"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:27:00.864313"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:27:14.258242"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:27:27.662909"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:27:41.075066"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:27:54.490310"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:28:07.900005"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:28:21.339336"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:28:34.768161"
    },
    {
      "challenge_name": "monte-carlo-integration",
      "challenge_title": "Easy\nMonte Carlo Integration\nImplement Monte Carlo integration on a GPU. Given a set of function values \\(y_i = f(x_i)\\) sampled at random points \\(x_i\\) uniformly distributed in the interval \\([a, b]\\), estimate the definite integral: \\[ \\int_a^b f(x) \\, dx \\approx (b - a) \\cdot \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] The Monte Carlo method approximates the integral by computing the average of the function values and multiplying by the interval width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the result variable Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2 Example: Input: a = 0, b = 2, n_samples = 8 y_samples = [0.0625, 0.25, 0.5625, 1.0, 1.5625, 2.25, 3.0625, 4.0] Output: result = 3.1875 Constraints 1 \u2264 n_samples \u2264 100,000,000 -1000.0 \u2264 a &lt; b \u2264 1000.0 -10000.0 \u2264 function values \u2264 10000.0 The tolerance is set to 1e-2 to account for the inherent randomness in Monte Carlo methods and floating-point precision variations.",
      "challenge_url": "https://leetgpu.com/challenges/monte-carlo-integration",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:28:48.184064"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7034 ms",
      "fastest_ms": 0.7034,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:29:19.835983"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1204 ms",
      "fastest_ms": 0.1204,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:29:33.247835"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0739 ms",
      "fastest_ms": 0.0739,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:29:46.662684"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.057 ms",
      "fastest_ms": 0.057,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:30:00.084761"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0469 ms",
      "fastest_ms": 0.0469,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:30:13.490171"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7294 ms",
      "fastest_ms": 0.7294,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:30:26.906700"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2046 ms",
      "fastest_ms": 0.2046,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:30:40.324672"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:30:53.749646"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:31:07.154897"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "310.049 ms",
      "fastest_ms": 310.049,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:31:20.573422"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7578 ms",
      "fastest_ms": 0.7578,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:31:34.001452"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:31:47.419550"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:32:00.825628"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:32:14.236953"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:32:27.640409"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.2525 ms",
      "fastest_ms": 2.2525,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:32:41.051794"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:32:54.459398"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:33:07.863885"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:33:21.271365"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:33:34.668026"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "6.0253 ms",
      "fastest_ms": 6.0253,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:33:48.084406"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "3.8891 ms",
      "fastest_ms": 3.8891,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:34:01.501007"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "3.151 ms",
      "fastest_ms": 3.151,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:34:14.915268"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:34:28.319010"
    },
    {
      "challenge_name": "reduction",
      "challenge_title": "Medium\nReduction\nWrite a CUDA program that performs parallel reduction on an array of 32-bit floating point numbers to compute their sum. The program should take an input array and produce a single output value containing the sum of all elements. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0] Output: 36.0 Example 2: Input: [-2.5, 1.5, -1.0, 2.0] Output: 0.0 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The final sum will always fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/reduction",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:34:41.724422"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0059 ms",
      "fastest_ms": 0.0059,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:35:13.424313"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0041 ms",
      "fastest_ms": 0.0041,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:35:26.863232"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0031 ms",
      "fastest_ms": 0.0031,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:35:40.283781"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0052 ms",
      "fastest_ms": 0.0052,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:35:53.722876"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0065 ms",
      "fastest_ms": 0.0065,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:36:07.161812"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.3697 ms",
      "fastest_ms": 0.3697,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:36:20.603219"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2231 ms",
      "fastest_ms": 0.2231,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:36:34.041613"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.5294 ms",
      "fastest_ms": 0.5294,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:36:47.469455"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:37:00.901323"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.2099 ms",
      "fastest_ms": 0.2099,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:37:14.341579"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0886 ms",
      "fastest_ms": 0.0886,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:37:27.766090"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0621 ms",
      "fastest_ms": 0.0621,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:37:41.176465"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0451 ms",
      "fastest_ms": 0.0451,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:37:54.609568"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:38:08.026150"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:38:21.447580"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.5963 ms",
      "fastest_ms": 0.5963,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:38:34.885964"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:38:48.313824"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.5474 ms",
      "fastest_ms": 0.5474,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:39:01.745593"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:39:15.170051"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:39:28.596199"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "6.5775 ms",
      "fastest_ms": 6.5775,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:39:42.021371"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "5.3775 ms",
      "fastest_ms": 5.3775,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:39:55.455967"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "5.1361 ms",
      "fastest_ms": 5.1361,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:40:08.907829"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:40:22.346918"
    },
    {
      "challenge_name": "softmax",
      "challenge_title": "Medium\nSoftmax\nWrite a program that computes the softmax function for an array of 32-bit floating-point numbers on a GPU. The softmax function is defined as follows: For an input array \\(x\\) of length \\(n\\), the softmax of \\(x\\), denoted \\(\\sigma(x)\\), is an array of length \\(n\\) where the \\(i\\)-th element is: \\(\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\) Your solution should handle potential overflow issues by using the \"max trick\". Subtract the maximum value of the input array from each element before exponentiation. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: [1.0, 2.0, 3.0], N = 3 Output: [0.090, 0.244, 0.665] (approximately) Example 2: Input: [-10.0, -5.0, 0.0, 5.0, 10.0], N = 5 Output: [2.04e-09, 3.04e-07, 4.51e-05, 6.69e-03, 9.93e-01] (approximately) Constraints 1 &le; N &le; 500,000",
      "challenge_url": "https://leetgpu.com/challenges/softmax",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:40:35.784096"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.1967 ms",
      "fastest_ms": 0.1967,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:41:07.464065"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0829 ms",
      "fastest_ms": 0.0829,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:41:20.883730"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0028 ms",
      "fastest_ms": 0.0028,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:41:34.311416"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:41:47.719964"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:42:01.142379"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.2988 ms",
      "fastest_ms": 0.2988,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:42:14.567690"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:42:27.983023"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:42:41.410525"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:42:54.834785"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1529 ms",
      "fastest_ms": 0.1529,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:43:08.251452"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.1055 ms",
      "fastest_ms": 0.1055,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:43:21.667445"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:43:35.087589"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0512 ms",
      "fastest_ms": 0.0512,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:43:48.502636"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:44:01.921031"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0596 ms",
      "fastest_ms": 0.0596,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:44:15.337170"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "8.148 ms",
      "fastest_ms": 8.148,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:44:28.754693"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:44:42.168068"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:44:55.602302"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:45:09.011675"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:45:22.430870"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "11.32 ms",
      "fastest_ms": 11.32,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:45:35.856027"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "9.3338 ms",
      "fastest_ms": 9.3338,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:45:49.271871"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "6.7708 ms",
      "fastest_ms": 6.7708,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:46:02.711254"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:46:16.115393"
    },
    {
      "challenge_name": "softmax-attention",
      "challenge_title": "Medium\nSoftmax Attention\nImplement a CUDA program that computes the softmax attention operation for a given set of matrices. Given the query matrix Q of size M\u00d7d, key matrix K of size N\u00d7d, and value matrix V of size N\u00d7d, your program should compute the output matrix using the formula: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\Bigl( \\frac{QK^T}{\\sqrt{d}} \\Bigr)V,$$ where the softmax function is applied row-wise. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output matrix output Example 1: Input: Q (2\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix} \\] K (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 & 0.0 \\end{bmatrix} \\] V (3\u00d74): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\\\ 9.0 & 10.0 & 11.0 & 12.0 \\end{bmatrix} \\] Output: output (2\u00d74): \\[ \\begin{bmatrix} 4.29 & 5.29 & 6.29 & 7.29 \\\\ 5.00 & 6.00 & 7.00 & 8.00 \\end{bmatrix} \\] Example 2: Input: Q (1\u00d72): \\[ \\begin{bmatrix} 1.0 & 2.0 \\end{bmatrix} \\] K (2\u00d72): \\[ \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} \\] V (2\u00d72): \\[ \\begin{bmatrix} 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Output: output (1\u00d72): \\[ \\begin{bmatrix} 4.34 & 5.34 \\end{bmatrix} \\] Constraints Matrix Q is of size M\u00d7d and matrices K and V are of size N\u00d7d 1 &le; M, N &le; 100,000 1 &le; d &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/softmax-attention",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:46:29.537821"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "5.1592 ms",
      "fastest_ms": 5.1592,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:47:01.233117"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.9294 ms",
      "fastest_ms": 0.9294,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:47:14.681854"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.6241 ms",
      "fastest_ms": 0.6241,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:47:28.105344"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:47:41.529638"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:47:54.955925"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "8.1549 ms",
      "fastest_ms": 8.1549,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:48:08.389017"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:48:21.815339"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.7494 ms",
      "fastest_ms": 0.7494,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:48:35.231274"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:48:48.638748"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "116.9996 ms",
      "fastest_ms": 116.9996,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:49:02.046523"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "7.8352 ms",
      "fastest_ms": 7.8352,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:49:15.468774"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:49:28.882003"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:49:42.287543"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:49:55.707575"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:50:09.117915"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "9.1379 ms",
      "fastest_ms": 9.1379,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:50:22.539154"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:50:35.949467"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:50:49.365110"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:51:02.784418"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:51:16.199914"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "76.5641 ms",
      "fastest_ms": 76.5641,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:51:29.604794"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "29.8518 ms",
      "fastest_ms": 29.8518,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:51:43.008916"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "13.0513 ms",
      "fastest_ms": 13.0513,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:51:56.420471"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:52:09.829897"
    },
    {
      "challenge_name": "2d-convolution",
      "challenge_title": "Medium\n2D Convolution\nWrite a program that performs a 2D convolution operation on the GPU. Given an input matrix and a kernel (filter), compute the convolved output. The convolution should be performed with a \"valid\" boundary condition, meaning the kernel is only applied where it fully overlaps with the input. The input consists of: input: A 2D matrix of 32-bit floating-point numbers, represented as a 1D array in row-major order. kernel: A 2D kernel (filter) of 32-bit floating-point numbers, also represented as a 1D array in row-major order. The output should be written to the output matrix (also a 1D array in row-major order). The output matrix will have dimensions: output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 The convolution operation is defined as: \\(output[i][j] = \\sum_{m=0}^{kernel\\_rows-1} \\sum_{n=0}^{kernel\\_cols-1} input[i+m][j+n] * kernel[m][n]\\) Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the array output Example 1: Input: input (3\u00d73): \\[ \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\] kernel (2\u00d72): \\[ \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\] input_rows = 3 input_cols = 3 kernel_rows = 2 kernel_cols = 2 Output: output (2\u00d72): \\[ \\begin{bmatrix} 6 & 8 \\\\ 12 & 14 \\end{bmatrix} \\] Example 2: Input: input (4\u00d74): \\[ \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 3 & 1 \\\\ 1 & 4 & 5 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\] kernel (1\u00d73): \\[ \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\] input_rows = 4 input_cols = 4 kernel_rows = 1 kernel_cols = 3 Output: output (4\u00d72): \\[ \\begin{bmatrix} 2 & 2 \\\\ 4 & 3 \\\\ 6 & 5 \\\\ 2 & 2 \\end{bmatrix} \\] Constraints 1 \u2264 input_rows, input_cols \u2264 3072 1 \u2264 kernel_rows, kernel_cols \u2264 31 kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/2d-convolution",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:52:23.240369"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7431 ms",
      "fastest_ms": 0.7431,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:52:54.905296"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1393 ms",
      "fastest_ms": 0.1393,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:53:08.316193"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.099 ms",
      "fastest_ms": 0.099,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:53:21.735948"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0833 ms",
      "fastest_ms": 0.0833,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:53:35.115610"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0614 ms",
      "fastest_ms": 0.0614,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:53:48.516215"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "22.4939 ms",
      "fastest_ms": 22.4939,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:54:01.933961"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:54:15.339749"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:54:28.742123"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:54:42.145946"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "11.2463 ms",
      "fastest_ms": 11.2463,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:54:55.559089"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.4088 ms",
      "fastest_ms": 1.4088,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:55:08.960590"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:55:22.348825"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:55:35.745539"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:55:49.148672"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:56:02.549643"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "3.2881 ms",
      "fastest_ms": 3.2881,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:56:15.945324"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:56:29.335252"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:56:42.713265"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:56:56.102246"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:57:09.505007"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:57:22.907296"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:57:36.299512"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:57:49.693612"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:58:03.090911"
    },
    {
      "challenge_name": "histogramming",
      "challenge_title": "Medium\nHistogramming\nWrite a GPU program that computes the histogram of an array of 32-bit integers. The histogram should count the number of occurrences of each integer value in the range [0, num_bins). You are given an input array input of length N and the number of bins num_bins. The result should be an array of integers of length num_bins, where each element represents the count of occurrences of its corresponding index in the input array. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the histogram array. Examples Input: input = [0, 1, 2, 1, 0], N = 5, num_bins = 3 Output: [2, 2, 1] Input: input = [3, 3, 3, 3], N = 4, num_bins = 5 Output: [0, 0, 0, 4, 0] Constraints 1 &le; N &le; 100,000,000 0 &le; input[i] &lt; num_bins 1 &le; num_bins &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/histogramming",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:58:16.491682"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.8779 ms",
      "fastest_ms": 0.8779,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:58:48.134847"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.4434 ms",
      "fastest_ms": 0.4434,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:59:01.529396"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "2.9571 ms",
      "fastest_ms": 2.9571,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:59:14.915945"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:59:28.320026"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:59:41.735426"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T02:59:55.159868"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:00:08.544779"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:00:21.928948"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:00:35.336114"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "4.8296 ms",
      "fastest_ms": 4.8296,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:00:48.731117"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.9329 ms",
      "fastest_ms": 0.9329,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:01:02.141170"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:01:15.527227"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:01:28.930051"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:01:42.336218"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1914 ms",
      "fastest_ms": 0.1914,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:01:55.734239"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "7.8236 ms",
      "fastest_ms": 7.8236,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:02:09.131180"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:02:22.534505"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:02:35.937131"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:02:49.339562"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:03:02.740777"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:03:16.144401"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:03:29.556786"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:03:42.955684"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:03:56.363165"
    },
    {
      "challenge_name": "sorting",
      "challenge_title": "Medium\nSorting\nWrite a program that sorts an array of 32-bit floating-point numbers in ascending order. You are free to choose any sorting algorithm. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The sorted result must be stored back in the input data array Example Input: data = [5.0, 2.0, 8.0, 1.0, 9.0, 4.0], N = 6 Output: data = [1.0, 2.0, 4.0, 5.0, 8.0, 9.0] Constraints 1 &le; N &le; 1,000,000",
      "challenge_url": "https://leetgpu.com/challenges/sorting",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:04:09.763823"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0285 ms",
      "fastest_ms": 0.0285,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:04:41.390209"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0174 ms",
      "fastest_ms": 0.0174,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:04:54.775793"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0162 ms",
      "fastest_ms": 0.0162,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:05:08.181733"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0191 ms",
      "fastest_ms": 0.0191,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:05:21.565682"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0202 ms",
      "fastest_ms": 0.0202,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:05:34.962449"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.9857 ms",
      "fastest_ms": 0.9857,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:05:48.366722"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:06:01.773823"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:06:15.179082"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:06:28.574148"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "1.3868 ms",
      "fastest_ms": 1.3868,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:06:41.956921"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0373 ms",
      "fastest_ms": 0.0373,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:06:55.334277"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:07:08.717671"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:07:22.119192"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:07:35.532978"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:07:48.934763"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.9508 ms",
      "fastest_ms": 0.9508,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:08:02.338681"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:08:15.729326"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:08:29.127285"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:08:42.530413"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:08:55.943892"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:09:09.352582"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:09:22.738896"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:09:36.123455"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:09:49.502678"
    },
    {
      "challenge_name": "prefix-sum",
      "challenge_title": "Medium\nPrefix Sum\nWrite a CUDA program that computes the prefix sum (cumulative sum) of an array of 32-bit floating point numbers. For an input array [a, b, c, d, ...], the prefix sum is [a, a+b, a+b+c, a+b+c+d, ...]. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The result must be stored in the output array Example 1: Input: [1.0, 2.0, 3.0, 4.0] Output: [1.0, 3.0, 6.0, 10.0] Example 2: Input: [5.0, -2.0, 3.0, 1.0, -4.0] Output: [5.0, 3.0, 6.0, 7.0, 3.0] Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; input[i] &le; 1000.0 The largest value in the output array will fit within a 32-bit float",
      "challenge_url": "https://leetgpu.com/challenges/prefix-sum",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:10:02.903229"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7 ms",
      "fastest_ms": 0.7,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:10:34.604308"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1308 ms",
      "fastest_ms": 0.1308,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:10:47.999656"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0751 ms",
      "fastest_ms": 0.0751,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:11:01.406650"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0575 ms",
      "fastest_ms": 0.0575,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:11:14.830893"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0449 ms",
      "fastest_ms": 0.0449,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:11:28.251430"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7561 ms",
      "fastest_ms": 0.7561,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:11:41.666833"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:11:55.069847"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "1.4452 ms",
      "fastest_ms": 1.4452,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:12:08.467880"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:12:21.867569"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "214.0048 ms",
      "fastest_ms": 214.0048,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:12:35.268191"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.782 ms",
      "fastest_ms": 0.782,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:12:48.665224"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:13:02.059086"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:13:15.471350"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:13:28.869828"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:13:42.290930"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.6131 ms",
      "fastest_ms": 1.6131,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:13:55.711628"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:14:09.112019"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:14:22.507733"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:14:35.905745"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:14:49.309219"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "6.3839 ms",
      "fastest_ms": 6.3839,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:15:02.712014"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "3.4412 ms",
      "fastest_ms": 3.4412,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:15:16.130816"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "2.6211 ms",
      "fastest_ms": 2.6211,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:15:29.538230"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:15:42.935192"
    },
    {
      "challenge_name": "dot-product",
      "challenge_title": "Medium\nDot Product\nImplement a CUDA program that computes the dot product of two vectors containing 32-bit floating point numbers. The dot product is the sum of the products of the corresponding elements of two vectors. Mathematically, the dot product of two vectors \\(A\\) and \\(B\\) of length \\(n\\) is defined as: \\[ A \\cdot B = \\sum_{i=0}^{n-1} A_i \\cdot B_i = A_0 \\cdot B_0 + A_1 \\cdot B_1 + \\ldots + A_{n-1} \\cdot B_{n-1} \\] Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output variable Example 1: Input: A = [1.0, 2.0, 3.0, 4.0] B = [5.0, 6.0, 7.0, 8.0] Output: result = 70.0 (1.0*5.0 + 2.0*6.0 + 3.0*7.0 + 4.0*8.0) Example 2: Input: A = [0.5, 1.5, 2.5] B = [2.0, 3.0, 4.0] Output: result = 16.0 (0.5*2.0 + 1.5*3.0 + 2.5*4.0) Constraints A and B have identical lengths 1 &le; N &le; 100,000,000",
      "challenge_url": "https://leetgpu.com/challenges/dot-product",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:15:56.339249"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.1884 ms",
      "fastest_ms": 0.1884,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:16:27.987083"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2834 ms",
      "fastest_ms": 0.2834,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:16:41.390671"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.9704 ms",
      "fastest_ms": 0.9704,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:16:54.820081"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:17:08.212504"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:17:21.615421"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "8.1483 ms",
      "fastest_ms": 8.1483,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:17:35.025034"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:17:48.424367"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:18:01.823561"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:18:15.236113"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "4.7421 ms",
      "fastest_ms": 4.7421,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:18:28.636109"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.19 ms",
      "fastest_ms": 0.19,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:18:42.047158"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:18:55.444339"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:19:08.829001"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:19:22.230893"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:19:35.634181"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.7204 ms",
      "fastest_ms": 0.7204,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:19:49.040321"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:20:02.446804"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:20:15.845808"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:20:29.250748"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:20:42.638276"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:20:56.032791"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:21:09.440568"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:21:22.838425"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:21:36.245355"
    },
    {
      "challenge_name": "sparse-matrix-vector-multiplication",
      "challenge_title": "Medium\nSparse Matrix-Vector Multiplication\nImplement a CUDA program that performs sparse matrix-vector multiplication. Given a sparse matrix \\(A\\) of dimensions \\(M \\times N\\) and a dense vector \\(x\\) of length \\(N\\), compute the product vector \\(y = A \\times x\\), which will have length \\(M\\). A is stored in row-major order. nnz is the number of non-zero elements in A. Mathematically, the operation is defined as: \\[ y_i = \\sum_{j=0}^{N-1} A_{ij} \\cdot x_j \\quad \\text{for} \\quad i = 0, 1, \\ldots, M-1 \\] The matrix \\(A\\) is approximately 60 - 70% sparse. Implementation Requirements Use only CUDA native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in vector y Example: Input: Matrix \\(A\\) (\\(3 \\times 4\\)): \\[ \\begin{bmatrix} 5.0 & 0.0 & 0.0 & 1.0 \\\\ 0.0 & 2.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 4.0 \\end{bmatrix} \\] Vector \\(x\\): \\[ \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\end{bmatrix} \\] Output: Vector \\(y\\): \\[ \\begin{bmatrix} 9.0 \\\\ 13.0 \\\\ 16.0 \\end{bmatrix} \\] Constraints 1 &le; M, N &le; 10,000 The matrix \\(A\\) is approximately 60-70% sparse (i.e., 60-70% of elements are zero)",
      "challenge_url": "https://leetgpu.com/challenges/sparse-matrix-vector-multiplication",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:21:49.656567"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.1164 ms",
      "fastest_ms": 1.1164,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:22:21.337346"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.5436 ms",
      "fastest_ms": 0.5436,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:22:34.803525"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1652 ms",
      "fastest_ms": 0.1652,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:22:48.220702"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.5371 ms",
      "fastest_ms": 0.5371,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:23:01.640631"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.3907 ms",
      "fastest_ms": 0.3907,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:23:15.050667"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:23:28.453423"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1225 ms",
      "fastest_ms": 0.1225,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:23:41.868213"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0612 ms",
      "fastest_ms": 0.0612,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:23:55.274935"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:24:08.686075"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0765 ms",
      "fastest_ms": 0.0765,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:24:22.098996"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.1821 ms",
      "fastest_ms": 0.1821,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:24:35.514662"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:24:48.929782"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:25:02.354920"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:25:15.759069"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:25:29.144535"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.9605 ms",
      "fastest_ms": 2.9605,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:25:42.558696"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:25:55.969433"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:26:09.381169"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:26:22.792051"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:26:36.198629"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:26:49.614564"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:27:03.026226"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:27:16.425096"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:27:29.827616"
    },
    {
      "challenge_name": "gemm-fp16",
      "challenge_title": "Medium\nGEMM (FP16)\nImplement a basic General Matrix Multiplication (GEMM). Given matrix \\(A\\) of dimensions \\(M \\times K\\), matrix \\(B\\) of dimensions \\(K \\times N\\), input/output matrix \\(C\\) of dimensions \\(M \\times N\\), and scalar multipliers \\( \\alpha \\) and \\( \\beta \\), compute the operation: \\[ C = \\alpha \\cdot (A \\times B) + \\beta \\cdot C_{initial} \\] The input matrices \\(A\\), \\(B\\), and the initial state of \\(C\\) contain 16-bit floating-point numbers (FP16/half). All matrices are stored in row-major order. The scalars \\( \\alpha \\) and \\( \\beta \\) are 32-bit floats. Implementation Requirements Use only native features (external libraries other than WMMA are not permitted). The solve function signature must remain unchanged. Accumulation during multiplication should use FP32 for better precision before converting the final result to FP16. The final result must be stored back into matrix C as half. Example: Input: (Note: Input matrices A, B, C_initial are FP16 type for the problem) Matrix \\(A\\) (\\(M=2, K=3\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(B\\) (\\(K=3, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\\\ 5.0 & 6.0 \\end{bmatrix} \\] Matrix \\(C_{initial}\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 1.0 & 1.0 \\\\ 1.0 & 1.0 \\end{bmatrix} \\] \\[\\alpha = 1.0 \\text{ (FP32)}\\] \\[\\beta = 0.0 \\text{ (FP32)}\\] Output (FP16): Matrix \\(C\\) (\\(M=2, N=2\\)): \\[ \\begin{bmatrix} 22.0 & 28.0 \\\\ 49.0 & 64.0 \\end{bmatrix} \\] Constraints 16 &le; M, N, K &le; 4096",
      "challenge_url": "https://leetgpu.com/challenges/gemm-fp16",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:27:43.219673"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.1817 ms",
      "fastest_ms": 0.1817,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:28:14.850556"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0411 ms",
      "fastest_ms": 0.0411,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:28:28.253579"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0284 ms",
      "fastest_ms": 0.0284,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:28:41.653041"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0309 ms",
      "fastest_ms": 0.0309,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:28:55.067172"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0273 ms",
      "fastest_ms": 0.0273,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:29:08.463956"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.2141 ms",
      "fastest_ms": 0.2141,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:29:21.902275"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:29:35.322998"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:29:48.731458"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:30:02.134215"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1604 ms",
      "fastest_ms": 0.1604,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:30:15.537694"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.833 ms",
      "fastest_ms": 0.833,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:30:28.931625"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.3716 ms",
      "fastest_ms": 0.3716,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:30:42.346793"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:30:55.756135"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:31:09.155079"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:31:22.557864"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "3.0428 ms",
      "fastest_ms": 3.0428,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:31:35.945942"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:31:49.349167"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:32:02.744653"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:32:16.141427"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:32:29.538912"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:32:42.962681"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:32:56.346688"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:33:09.747929"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:33:23.146470"
    },
    {
      "challenge_name": "categorical-cross-entropy-loss",
      "challenge_title": "Medium\nCategorical Cross-Entropy Loss\nImplement a CUDA program to calculate the categorical cross-entropy loss for a batch of predictions. Given a matrix of predicted logits \\(Z\\) of size \\(N \\times C\\) and a vector of true class labels true_labels of size \\(N\\), compute the average cross-entropy loss over the batch. The loss for a single sample \\(j\\) with logits \\(z_j = [z_{j1}, \\ldots, z_{jC}]\\) and true label \\(y_j\\) is calculated using the numerically stable formula: \\[ \\text{Loss}_j = \\log\\left(\\sum_{k=1}^{C} e^{z_{jk}}\\right) - z_{j, y_j} \\] The final output stored in the loss variable should be the average loss over the \\(N\\) samples: \\[ L = \\frac{1}{N} \\sum_{j=1}^{N} \\text{Loss}_j \\] The input parameters are logits, true_labels, N (number of samples), and C (number of classes). The result should be stored in loss (a pointer to a single float). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result (average loss) must be stored in loss Example 1: Input: N = 2, C = 3 logits = [[1.0, 2.0, 0.5], [0.1, 3.0, 1.5]] true_labels = [1, 1] Output: loss = [0.3548926] Example 2: Input: N = 3, C = 4 logits = [[-0.5, 1.5, 0.0, 1.0], [2.0, -1.0, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0]] true_labels = [3, 0, 1] Output: loss = [0.98820376] Constraints 1 &le; N &le; 10,000 2 &le; C &le; 1,000 -10.0 &le; logits[i, j] &le; 10.0 0 &le; true_labels[i] &le; C",
      "challenge_url": "https://leetgpu.com/challenges/categorical-cross-entropy-loss",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:33:36.556164"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0033 ms",
      "fastest_ms": 0.0033,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:34:08.263110"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0028 ms",
      "fastest_ms": 0.0028,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:34:21.715977"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0035 ms",
      "fastest_ms": 0.0035,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:34:35.155156"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0032 ms",
      "fastest_ms": 0.0032,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:34:48.610976"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.004 ms",
      "fastest_ms": 0.004,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:35:02.061881"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:35:15.530035"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:35:28.960179"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:35:42.400470"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:35:55.837142"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.004 ms",
      "fastest_ms": 0.004,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:36:09.265340"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:36:22.694178"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:36:36.117701"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:36:49.532629"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:37:02.965853"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:37:16.394748"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:37:29.817789"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:37:43.252035"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:37:56.689286"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:38:10.122405"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:38:23.539406"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:38:36.976192"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:38:50.401194"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:39:03.820474"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:39:17.267195"
    },
    {
      "challenge_name": "password-cracking-fnv-1a",
      "challenge_title": "Medium\nPassword Cracking (FNV-1a)\nImplement a parallel brute-force password cracker. Given a target hash value, find the original password (composed of lowercase English letters) of a specific length which, when hashed R times using the provided hash function, produces the target hash. Your task is to search through all possible passwords of the given password_length containing only lowercase English letters ('a' through 'z') until you find the one that produces the target_hash after R rounds of hashing. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The found password must be written to the output_password array, followed by a null terminator Example 1: Input: target_hash = 537089824 password_length = 3 R = 2 Output: output_password = \"abc\" Example 2: Input: target_hash = 440920331 password_length = 3 R = 1 Output: output_password = \"abc\" Constraints 1 \u2264 password_length \u2264 8 1 \u2264 R \u2264 100 The target hash corresponds to a unique password within the search space defined by the length and lowercase alphabet",
      "challenge_url": "https://leetgpu.com/challenges/password-cracking-fnv-1a",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:39:30.717947"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.3925 ms",
      "fastest_ms": 1.3925,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:40:02.373683"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2246 ms",
      "fastest_ms": 0.2246,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:40:15.791713"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1364 ms",
      "fastest_ms": 0.1364,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:40:29.198216"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": "0.0998 ms",
      "fastest_ms": 0.0998,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:40:42.598673"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.071 ms",
      "fastest_ms": 0.071,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:40:56.023166"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.4332 ms",
      "fastest_ms": 1.4332,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:41:09.426700"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:41:22.811903"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:41:36.201781"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:41:49.601541"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1295 ms",
      "fastest_ms": 0.1295,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:42:03.001605"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "3.1725 ms",
      "fastest_ms": 3.1725,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:42:16.396083"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:42:29.795349"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:42:43.200055"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:42:56.607921"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:43:09.999617"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.2364 ms",
      "fastest_ms": 2.2364,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:43:23.407613"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:43:36.811922"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:43:50.213785"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:44:03.600377"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:44:16.998459"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:44:30.401250"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:44:43.790096"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:44:57.190916"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:45:10.593195"
    },
    {
      "challenge_name": "mean-squared-error",
      "challenge_title": "Medium\nMean Squared Error\nImplement a CUDA program to calculate the Mean Squared Error (MSE) between predicted values and target values. Given two arrays of equal length, predictions and targets, compute: \\[ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (predictions_i - targets_i)^2 \\] where N is the number of elements in each array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the mse variable Example 1: Input: predictions = [1.0, 2.0, 3.0, 4.0] targets = [1.5, 2.5, 3.5, 4.5] Output: mse = 0.25 Example 2: Input: predictions = [10.0, 20.0, 30.0] targets = [12.0, 18.0, 33.0] Output: mse = 5.67 Constraints 1 &le; N &le; 100,000,000 -1000.0 &le; predictions[i], targets[i] &le; 1000.0",
      "challenge_url": "https://leetgpu.com/challenges/mean-squared-error",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:45:24.017028"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.0896 ms",
      "fastest_ms": 0.0896,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:45:55.681297"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.0252 ms",
      "fastest_ms": 0.0252,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:46:09.088450"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.0228 ms",
      "fastest_ms": 0.0228,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:46:22.512007"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:46:35.910950"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:46:49.332771"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "14.5911 ms",
      "fastest_ms": 14.5911,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:47:02.742180"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:47:16.159953"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:47:29.567855"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:47:42.977786"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.0448 ms",
      "fastest_ms": 0.0448,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:47:56.387817"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.1469 ms",
      "fastest_ms": 0.1469,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:48:09.799934"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:48:23.202601"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:48:36.629149"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:48:50.033613"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:49:03.441731"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.2054 ms",
      "fastest_ms": 0.2054,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:49:16.857567"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:49:30.261549"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:49:43.657990"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:49:57.058862"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:50:10.464533"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:50:23.867052"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:50:37.273404"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:50:50.670736"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:51:04.080090"
    },
    {
      "challenge_name": "gaussian-blur",
      "challenge_title": "Medium\nGaussian Blur\nImplement a program that applies a Gaussian blur filter to a 2D image. Given an input image represented as a floating-point array and a Gaussian kernel, the program should compute the convolution of the image with the kernel. All inputs and outputs are stored in row-major order. The Gaussian blur is performed by convolving each pixel with a weighted average of its neighbors, where the weights are determined by the Gaussian kernel. For each output pixel at position (i, j), the value is calculated as: \\[ output[i, j] = \\sum_{m=-k_h/2}^{k_h/2} \\sum_{n=-k_w/2}^{k_w/2} input[i+m, j+n] \\times kernel[m+k_h/2, n+k_w/2] \\] where \\(k_h\\) and \\(k_w\\) are the kernel height and width. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Handle boundary conditions by using zero-padding (treat values outside the image boundary as zeros) Example 1: Input: image (5\u00d75) = [ [1.0, 2.0, 3.0, 4.0, 5.0], [6.0, 7.0, 8.0, 9.0, 10.0], [11.0, 12.0, 13.0, 14.0, 15.0], [16.0, 17.0, 18.0, 19.0, 20.0], [21.0, 22.0, 23.0, 24.0, 25.0] ] kernel (3\u00d73) = [ [0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625] ] Output: output (5\u00d75) = [ [1.6875, 2.75, 3.5, 4.25, 3.5625], [4.75, 7.0, 8.0, 9.0, 7.25], [8.5, 12.0, 13.0, 14.0, 11.0], [12.25, 17.0, 18.0, 19.0, 14.75], [11.0625, 15.25, 16.0, 16.75, 12.9375] ] Example 2: Input: image (3\u00d73) = [ [10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [70.0, 80.0, 90.0] ] kernel (3\u00d73) = [ [0.1, 0.1, 0.1], [0.1, 0.2, 0.1], [0.1, 0.1, 0.1] ] Output: output (3\u00d73) = [ [13.0, 23.0, 19.0], [31.0, 50.0, 39.0], [31.0, 47.0, 37.0] ] Constraints 1 \u2264 input_rows, input_cols \u2264 4096 3 \u2264 kernel_rows, kernel_cols \u2264 21 Both kernel_rows and kernel_cols will be odd numbers All kernel values will be non-negative and sum to 1.0 (normalized)",
      "challenge_url": "https://leetgpu.com/challenges/gaussian-blur",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:51:17.484029"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "19.0976 ms",
      "fastest_ms": 19.0976,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:51:49.159890"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "4.201 ms",
      "fastest_ms": 4.201,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:52:02.560897"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "3.0171 ms",
      "fastest_ms": 3.0171,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:52:15.977443"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:52:29.379297"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:52:42.782147"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:52:56.183495"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:53:09.579292"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:53:22.963100"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:53:36.353200"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "737.4223 ms",
      "fastest_ms": 737.4223,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:53:49.748914"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "5.5594 ms",
      "fastest_ms": 5.5594,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:54:03.139830"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:54:16.529662"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:54:29.917728"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:54:43.327433"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:54:56.714525"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "260.5104 ms",
      "fastest_ms": 260.5104,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:55:10.125680"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:55:23.530996"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:55:36.936542"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:55:50.346906"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:56:03.752333"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:56:17.154663"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:56:30.547426"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:56:43.951252"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:56:57.354402"
    },
    {
      "challenge_name": "top-k-selection",
      "challenge_title": "Medium\nTop-K Selection\nImplement a GPU program that, given a 1D array input of 32-bit floating point numbers of length N, selects the k largest elements and writes them in descending order to the output array of length k. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: input = [1.0, 5.0, 3.0, 2.0, 4.0] N = 5 k = 3 Output: output = [5.0, 4.0, 3.0] Example 2: Input: input = [7.2, -1.0, 3.3, 8.8, 2.2] N = 5 k = 2 Output: output = [8.8, 7.2] Constraints 1 \u2264 N \u2264 100,000,000 1 \u2264 k \u2264 N All values in input are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/top-k-selection",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:57:10.748112"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.6898 ms",
      "fastest_ms": 1.6898,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:57:42.417534"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.2178 ms",
      "fastest_ms": 0.2178,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:57:55.812412"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1408 ms",
      "fastest_ms": 0.1408,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:58:09.213081"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:58:22.619279"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:58:36.010463"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.6089 ms",
      "fastest_ms": 0.6089,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:58:49.419845"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:59:02.825349"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:59:16.221285"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:59:29.625814"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1453 ms",
      "fastest_ms": 0.1453,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:59:43.039869"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.3517 ms",
      "fastest_ms": 0.3517,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T03:59:56.435667"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.1417 ms",
      "fastest_ms": 0.1417,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:00:09.830678"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:00:23.240361"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:00:36.640539"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:00:50.045100"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.8484 ms",
      "fastest_ms": 1.8484,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:01:03.447420"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:01:16.845262"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:01:30.248527"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:01:43.666983"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:01:57.063620"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:02:10.461325"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:02:23.873703"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:02:37.286281"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:02:50.700292"
    },
    {
      "challenge_name": "batched-matrix-multiplication-fp32",
      "challenge_title": "Medium\nBatched Matrix Multiplication (FP32)\nImplement a batched matrix multiplication in FP32. Given a batch of matrices A of shape [B, M, K] and a batch of matrices B of shape [B, K, N], compute the output batch C of shape [B, M, N] such that for each batch index b: \\[ C_b = A_b \\times B_b \\] All matrices are stored in row-major order and use 32-bit floating point numbers (FP32). Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the C array Example 1: Input: B = 2, M = 2, K = 3, N = 2 A = [ [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]] ] B = [ [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], [[6.0, 5.0], [4.0, 3.0], [2.0, 1.0]] ] Output: C = [ [[22.0, 28.0], [49.0, 64.0]], [[92.0, 68.0], [128.0, 95.0]] ] Constraints 1 &le; B &le; 128 1 &le; M, N, K &le; 1024",
      "challenge_url": "https://leetgpu.com/challenges/batched-matrix-multiplication-fp32",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:03:04.103951"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "131.0875 ms",
      "fastest_ms": 131.0875,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:03:35.748342"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "24.9964 ms",
      "fastest_ms": 24.9964,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:03:49.164792"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "28.3293 ms",
      "fastest_ms": 28.3293,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:04:02.578510"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:04:15.976374"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:04:29.386734"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:04:42.828797"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:04:56.228558"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": "1.7499 ms",
      "fastest_ms": 1.7499,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:05:09.649516"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:05:23.039252"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": "1.4819 ms",
      "fastest_ms": 1.4819,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:05:36.459531"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "34.5677 ms",
      "fastest_ms": 34.5677,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:05:49.884800"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:06:03.288632"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:06:16.690781"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:06:30.098146"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:06:43.506315"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "188.3359 ms",
      "fastest_ms": 188.3359,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:06:56.930419"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:07:10.340687"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:07:23.755201"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:07:37.147042"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:07:50.549231"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:08:03.962547"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:08:17.365369"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:08:30.782195"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:08:44.190468"
    },
    {
      "challenge_name": "quantized-matrix-multiplication-int8",
      "challenge_title": "Medium\nQuantized Matrix Multiplication (INT8)\nImplement a quantized matrix multiplication program for 8-bit signed integer matrices. Given two input matrices A of dimensions \\(M \\times K\\) and B of dimensions \\(K \\times N\\), quantization scales scale_A, scale_B, output scale scale_C, zero-points zero_point_A, zero_point_B, zero_point_C, compute: \\[ C_{\\text{quant}}(i, j) = \\mathrm{clamp}\\left( \\mathrm{round}\\left( \\frac{ \\sum_{k=0}^{K-1} (A_{ik} - z_A)(B_{kj} - z_B) \\cdot s_A s_B }{s_C} \\right) + z_C,\\ -128,\\ 127 \\right) \\] where s_A = scale_A, z_A = zero_point_A, etc. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final result must be stored in the output matrix C as int8 After accumulation in int32 and scaling in float32, values must be rounded to the nearest integer, shifted by zero_point_C, and clamped to the [-128, 127] range Example 1: Input: A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] M = 2, N = 2, K = 2 scale_A = 0.1, scale_B = 0.2, scale_C = 0.05 zero_point_A = 0, zero_point_B = 0, zero_point_C = 0 Output: C = [[19, 22], [43, 50]] Example 2: Input: A = [[1, 2]] B = [[3], [4]] M = 1, N = 1, K = 2 scale_A = 1.0, scale_B = 1.0, scale_C = 1.0 zero_point_A = 1, zero_point_B = 3, zero_point_C = 5 Output: C = [[6]] Constraints 1 \u2264 M, N, K \u2264 4096 scale_A, scale_B, scale_C are positive floats -128 \u2264 zero_point_A, zero_point_B, zero_point_C \u2264 127",
      "challenge_url": "https://leetgpu.com/challenges/quantized-matrix-multiplication-int8",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:08:57.596345"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.155 ms",
      "fastest_ms": 0.155,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:09:29.252899"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:09:42.686919"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.1179 ms",
      "fastest_ms": 0.1179,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:09:56.081361"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:10:09.489580"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "0.1366 ms",
      "fastest_ms": 0.1366,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:10:22.913029"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:10:36.334616"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:10:49.732423"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:11:03.138343"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:11:16.538903"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:11:29.960865"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.3222 ms",
      "fastest_ms": 0.3222,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:11:43.379282"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:11:56.789207"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:12:10.193815"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:12:23.594375"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:12:37.006107"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.6671 ms",
      "fastest_ms": 0.6671,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:12:50.421698"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:13:03.817696"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:13:17.223777"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:13:30.617131"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:13:44.013319"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:13:57.428424"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:14:10.830562"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:14:24.263179"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:14:37.660201"
    },
    {
      "challenge_name": "ordinary-least-squares-regression",
      "challenge_title": "Medium\nOrdinary Least Squares Regression\nSolve the Ordinary Least Squares (OLS) regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a target vector \\(y\\) of size \\(n\\_samples\\), compute the coefficient vector \\(\\beta\\) that minimizes the sum of squared residuals: \\[ \\min_{\\beta} ||X\\beta - y||^2 \\] The closed-form solution to OLS is: \\[ \\beta = (X^TX)^{-1}X^Ty \\] Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector Assume that the feature matrix \\(X\\) is full rank (i.e., \\(X^TX\\) is invertible) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} -0.23 & -0.23 & 1.52 \\\\ 0.77 & -0.47 & 1.58 \\\\ -0.14 & 0.65 & 0.5 \\\\ -1.91 & -1.72 & 0.24 \\\\ -0.46 & -0.47 & 0.54 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 83.01 \\\\ 93.4 \\\\ 47.33 \\\\ -62.22 \\\\ 13.06 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 13.97 \\\\ 29.12 \\\\ 61.05 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -1000.0 \u2264 values in X and y \u2264 1000.0 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/ordinary-least-squares-regression",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:14:51.061103"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:15:22.734140"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:15:36.168645"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:15:49.583989"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:16:02.997872"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:16:16.406363"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:16:29.811177"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:16:43.195409"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:16:56.566661"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:17:09.949343"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:17:23.334842"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:17:36.719115"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:17:50.127696"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:18:03.531680"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:18:16.939005"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:18:30.338217"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:18:43.736614"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:18:57.143418"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:19:10.548131"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:19:23.956482"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:19:37.361816"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:19:50.772382"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:20:04.181255"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:20:17.574195"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:20:30.955688"
    },
    {
      "challenge_name": "logistic-regression",
      "challenge_title": "Medium\nLogistic Regression\nSolve the logistic regression problem on a GPU. Given a feature matrix \\(X\\) of size \\(n\\_samples \\times n\\_features\\) and a binary target vector \\(y\\) of size \\(n\\_samples\\) (containing only 0s and 1s), compute the coefficient vector \\(\\beta\\) that maximizes the log-likelihood: \\[ \\max_{\\beta} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right] \\] where \\(p_i = \\sigma(X_i^T \\beta)\\) and \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final coefficients must be stored in the beta vector The target vector y contains only binary values (0 and 1) Example: Input: \\(X\\) (samples \u00d7 features): \\[ \\begin{bmatrix} 2.0 & 1.0 \\\\ 1.0 & 2.0 \\\\ 3.0 & 3.0 \\\\ 1.5 & 2.5 \\\\ -1.0 & -2.0 \\\\ -2.0 & -1.0 \\\\ -1.5 & -2.5 \\\\ -3.0 & -3.0 \\end{bmatrix} \\] \\(y\\): \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] Output: \\(\\beta\\): \\[ \\begin{bmatrix} 2.26 \\\\ -1.29 \\end{bmatrix} \\] Constraints 1 \u2264 n_samples \u2264 100,000 1 \u2264 n_features \u2264 1,000 n_samples \u2265 n_features -10.0 \u2264 values in X \u2264 10.0 y contains only binary values: 0 or 1 Solutions are tested with absolute tolerance of 1e-2 and relative tolerance of 1e-2",
      "challenge_url": "https://leetgpu.com/challenges/logistic-regression",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:20:44.324183"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:21:15.979432"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:21:29.396157"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:21:42.786901"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:21:56.183549"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:22:09.592803"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:22:23.003136"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:22:36.414054"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:22:49.818635"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:23:03.220510"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:23:16.608688"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:23:30.012201"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:23:43.405894"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:23:56.793093"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:24:10.193137"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:24:23.584282"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:24:36.980225"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:24:50.384330"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:25:03.758905"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:25:17.156038"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:25:30.562669"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:25:43.964096"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:25:57.362036"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:26:10.762698"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:26:24.161598"
    },
    {
      "challenge_name": "radix-sort",
      "challenge_title": "Medium\nRadix Sort\nImplement a radix sort algorithm that sorts an array of 32-bit unsigned integers on a GPU. The program should take an input array of unsigned integers and sort them in ascending order using the radix sort algorithm. The input parameter contains the unsorted array, and the sorted result should be stored in the output array. Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged The final sorted result must be stored in the output array Use radix sort algorithm (not other sorting algorithms) Sort in ascending order Example 1: Input: [170, 45, 75, 90, 2, 802, 24, 66] Output: [2, 24, 45, 66, 75, 90, 170, 802] Example 2: Input: [1, 4, 1, 3, 555, 1000, 2] Output: [1, 1, 2, 3, 4, 555, 1000] Constraints 1 \u2264 N \u2264 100,000,000 0 \u2264 input[i] \u2264 4,294,967,295 (32-bit unsigned integers)",
      "challenge_url": "https://leetgpu.com/challenges/radix-sort",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:26:37.564019"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.6966 ms",
      "fastest_ms": 1.6966,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:27:09.193931"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "1.2968 ms",
      "fastest_ms": 1.2968,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:27:22.590374"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.4386 ms",
      "fastest_ms": 0.4386,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:27:35.987301"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:27:49.394169"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:28:02.786809"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:28:16.194110"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:28:29.625428"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:28:43.035622"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:28:56.429645"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:29:09.822701"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "0.321 ms",
      "fastest_ms": 0.321,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:29:23.217396"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:29:36.605731"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:29:50.008398"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:30:03.429452"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:30:16.815170"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:30:30.208909"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:30:43.623457"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:30:57.021688"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:31:10.424638"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:31:23.817872"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:31:37.218079"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:31:50.618108"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:32:04.019146"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:32:17.418395"
    },
    {
      "challenge_name": "matrix-power",
      "challenge_title": "Medium\nMatrix Power\nImplement a GPU program that raises a square matrix \\(A\\) of size \\(N \\times N\\) to an integer power \\(P\\). The solve function receives a flattened input matrix input (row-major order), an empty output matrix output of the same size, the dimension N, and the exponent P. You must compute \\(\\text{output} = A^{P}\\) where matrix multiplication is standard dense multiplication over 32-bit floating point numbers. Implementation Requirements External libraries are not permitted. The solve function signature must remain unchanged. The final result must be written to the output array in row-major order. Example 1: Input: input = [[1.0, 2.0], [3.0, 4.0]] N = 2 P = 3 Output: output = [[37.0, 54.0], [81.0, 118.0]] Example 2: Input: input = [[1.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 0.0]] N = 3 P = 2 Output: output = [[7.0, 0.0, 2.0], [0.0, 1.0, 0.0], [3.0, 0.0, 6.0]] Constraints \\(1 \\le N \\le 1024\\) \\(1 \\le P \\le 20\\) Elements of input satisfy \\(-10.0 \\le A_{ij} \\le 10.0\\)",
      "challenge_url": "https://leetgpu.com/challenges/matrix-power",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:32:30.793825"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.5214 ms",
      "fastest_ms": 1.5214,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:33:02.442584"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "4.1894 ms",
      "fastest_ms": 4.1894,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:33:15.854870"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:33:29.266812"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:33:42.685390"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:33:56.096327"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:34:09.505519"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.5085 ms",
      "fastest_ms": 0.5085,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:34:22.916535"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:34:36.340530"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:34:49.753810"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:35:03.151096"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "11.9563 ms",
      "fastest_ms": 11.9563,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:35:16.578193"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:35:30.001058"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:35:43.396814"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:35:56.784300"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:36:10.189006"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "4.0909 ms",
      "fastest_ms": 4.0909,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:36:23.613303"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:36:37.035858"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:36:50.442002"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:37:03.844079"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:37:17.225441"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "14.073 ms",
      "fastest_ms": 14.073,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:37:30.629143"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "5.7844 ms",
      "fastest_ms": 5.7844,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:37:44.024648"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": "5.0419 ms",
      "fastest_ms": 5.0419,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:37:57.443072"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:38:10.837457"
    },
    {
      "challenge_name": "3d-convolution",
      "challenge_title": "Hard\n3D Convolution\nImplement a program that performs a 3D convolution operation. Given a 3D input volume and a 3D kernel (filter), compute the convolved output. The convolution should use a \"valid\" boundary condition (no padding). For a 3D convolution, the output at position \\((i,j,k)\\) is given by: \\[ output(i,j,k) = \\sum_{d=0}^{K_d-1} \\sum_{r=0}^{K_r-1} \\sum_{c=0}^{K_c-1} input(i+d,j+r,k+c) \\cdot kernel(d,r,c) \\] The input consists of: input: A 3D volume of 32-bit floats, as a 1D array (row-major, then depth). kernel: A 3D kernel of 32-bit floats, as a 1D array (row-major, then depth). input_depth, input_rows, input_cols: Dimensions of the input. kernel_depth, kernel_rows, kernel_cols: Dimensions of the kernel. Output: output: A 1D array (row-major, then depth) storing the result. Output dimensions: output_depth = input_depth - kernel_depth + 1 output_rows = input_rows - kernel_rows + 1 output_cols = input_cols - kernel_cols + 1 Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in output Examples Example 1: Input volume \\(V \\in \\mathbb{R}^{3 \\times 3 \\times 3}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 10 & 11 & 12 \\\\ 13 & 14 & 15 \\\\ 16 & 17 & 18 \\end{bmatrix} \\\\ V_{d=2} &= \\begin{bmatrix} 19 & 20 & 21 \\\\ 22 & 23 & 24 \\\\ 25 & 26 & 27 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 3 \\times 3}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{2 \\times 1 \\times 1}\\): \\[ [44, 62] \\] Example 2: Input volume \\(V \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} V_{d=0} &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\\\ V_{d=1} &= \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} \\end{aligned} \\] Kernel \\(K \\in \\mathbb{R}^{2 \\times 2 \\times 2}\\): \\[ \\begin{aligned} K_{d=0} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\\\ K_{d=1} &= \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\end{aligned} \\] Output \\(O \\in \\mathbb{R}^{1 \\times 1 \\times 1}\\): \\[ [28] \\] Constraints 1 \u2264 input_depth, input_rows, input_cols \u2264 256 1 \u2264 kernel_depth, kernel_rows, kernel_cols \u2264 5 kernel_depth \u2264 input_depth kernel_rows \u2264 input_rows kernel_cols \u2264 input_cols",
      "challenge_url": "https://leetgpu.com/challenges/3d-convolution",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:38:24.238956"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "5.1994 ms",
      "fastest_ms": 5.1994,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:38:55.943312"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "6.9017 ms",
      "fastest_ms": 6.9017,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:39:09.337867"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "4.8871 ms",
      "fastest_ms": 4.8871,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:39:22.743150"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:39:36.149559"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:39:49.553532"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:40:02.960842"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:40:16.371659"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:40:29.778117"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:40:43.174581"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:40:56.573716"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.5324 ms",
      "fastest_ms": 2.5324,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:41:09.983535"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": "0.8518 ms",
      "fastest_ms": 0.8518,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:41:23.370982"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:41:36.750308"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:41:50.160423"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:42:03.572558"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "82.3378 ms",
      "fastest_ms": 82.3378,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:42:16.981575"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:42:30.374563"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:42:43.772189"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:42:57.189101"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:43:10.598950"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:43:24.013291"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:43:37.426824"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:43:50.817131"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:44:04.227713"
    },
    {
      "challenge_name": "multi-head-self-attention",
      "challenge_title": "Hard\nMulti-Head Self-Attention\nImplement a program for multi-head self-attention. Given three input matrices \\(Q\\) (queries), \\(K\\) (keys), and \\(V\\) (values) of size \\(N \\times d_{\\text{model}}\\), compute: \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h) \\] where each head computes: \\[ \\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i \\] with \\(d_k = d_{\\text{model}}/h\\) and \\(Q_i, K_i, V_i\\) being the i-th head's partition of the input matrices. Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the output array Example 1: Input: \\[ \\begin{align*} N &= 2, \\quad d_{\\text{model}} = 4, \\quad h = 2 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.39 & 2.89 & 3.50 & 4.00 \\\\ 2.50 & 3.00 & 3.50 & 4.00 \\end{bmatrix} \\] Example 2: Input: \\[ \\begin{align*} N &= 1, \\quad d_{\\text{model}} = 2, \\quad h = 1 \\\\[1em] Q &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] K &= \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix} \\\\[1em] V &= \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\end{align*} \\] Output: \\[ \\begin{bmatrix} 2.0 & 3.0 \\end{bmatrix} \\] Constraints 1 \u2264 N \u2264 10000 2 \u2264 d_model \u2264 1024 1 \u2264 h \u2264 d_model d_model % h == 0 -10.0 \u2264 values \u2264 10.0",
      "challenge_url": "https://leetgpu.com/challenges/multi-head-self-attention",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:44:17.629717"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.561 ms",
      "fastest_ms": 1.561,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:44:49.282991"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:45:02.716185"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:45:16.109996"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:45:29.521790"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:45:42.915991"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:45:56.312446"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:46:09.711507"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:46:23.111518"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:46:36.508339"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:46:49.893739"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:47:03.283725"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:47:16.673423"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:47:30.071254"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:47:43.465906"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:47:56.845101"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "2.7785 ms",
      "fastest_ms": 2.7785,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:48:10.245905"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:48:23.646987"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:48:37.047777"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:48:50.455654"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:49:03.843339"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:49:17.236752"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:49:30.629058"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:49:44.019856"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:49:57.419531"
    },
    {
      "challenge_name": "swarm-intelligence-flocking-simulation",
      "challenge_title": "Hard\nSwarm Intelligence & Flocking Simulation\nImplement a program for a multi-agent flocking simulation (boids). The input consists of: An array agents containing N agents, where N is the total number of agents Each agent occupies 4 consecutive 32-bit floating point numbers in the array: \\([x, y, v_x, v_y]\\), where: \\((x, y)\\) represents the agent's position in 2D space \\((v_x, v_y)\\) represents the agent's velocity vector The total array size is 4 * N floats, with agent \\(i\\)'s data stored at indices [4i, 4i+1, 4i+2, 4i+3] Simulation Rules For each agent \\(i\\), identify all neighbors \\(j\\) within radius \\(r = 5.0\\) using: \\[ \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \\leq r \\] Compute average velocity of neighboring agents: \\[ \\vec{v}_{avg} = \\frac{1}{|N_i|} \\sum_{j \\in N_i} \\vec{v}_j \\] where \\(N_i\\) is the set of neighbors for agent \\(i\\) Update velocity: \\[ \\vec{v}_{new} = \\vec{v} + \\alpha(\\vec{v}_{avg} - \\vec{v}), \\text{ where } \\alpha = 0.05 \\] Update position: \\[ \\vec{p}_{new} = \\vec{p} + \\vec{v}_{new} \\] Implementation Requirements Use only native features (external libraries are not permitted) The solve function signature must remain unchanged The final result must be stored in the agents_next array Example 1: Input: N = 2 agents = [ 0.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 4.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Output: agents_next = [ 1.0, 0.0, 1.0, 0.0, // Agent 0: [x, y, vx, vy] 3.0, 3.0, 0.0, -1.0 // Agent 1: [x, y, vx, vy] ] Constraints 1 &le; N &le; 100,000 Each agent's position and velocity components are 32-bit floats",
      "challenge_url": "https://leetgpu.com/challenges/swarm-intelligence-flocking-simulation",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:50:10.817341"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 1,
      "framework": "CUDA",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "1.2373 ms",
      "fastest_ms": 1.2373,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:50:42.476226"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 2,
      "framework": "CUDA",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:50:55.886764"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 3,
      "framework": "CUDA",
      "gpu": "NVIDIA H100",
      "fastest_time": "0.9147 ms",
      "fastest_ms": 0.9147,
      "total_timings_found": 3,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:51:09.313786"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 4,
      "framework": "CUDA",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:51:22.731808"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 5,
      "framework": "CUDA",
      "gpu": "NVIDIA B200",
      "fastest_time": "1.0154 ms",
      "fastest_ms": 1.0154,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:51:36.149173"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 6,
      "framework": "TRITON",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:51:49.567786"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 7,
      "framework": "TRITON",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:52:02.973422"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 8,
      "framework": "TRITON",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:52:16.379024"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 9,
      "framework": "TRITON",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:52:29.776295"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 10,
      "framework": "TRITON",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:52:43.181802"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 11,
      "framework": "PYTORCH",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "12.3074 ms",
      "fastest_ms": 12.3074,
      "total_timings_found": 2,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:52:56.597380"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 12,
      "framework": "PYTORCH",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:53:10.015833"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 13,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:53:23.418722"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 14,
      "framework": "PYTORCH",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:53:36.836242"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 15,
      "framework": "PYTORCH",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:53:50.247321"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 16,
      "framework": "MOJO",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": "4.4588 ms",
      "fastest_ms": 4.4588,
      "total_timings_found": 1,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:54:03.653481"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 17,
      "framework": "MOJO",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:54:17.057110"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 18,
      "framework": "MOJO",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:54:30.464586"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 19,
      "framework": "MOJO",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:54:43.865128"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 20,
      "framework": "MOJO",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:54:57.273824"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 21,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA TESLA T4",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:55:10.698117"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 22,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA A100-80GB",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:55:24.115815"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 23,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H100",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:55:37.536757"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 24,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA H200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:55:50.955653"
    },
    {
      "challenge_name": "k-means-clustering",
      "challenge_title": "Hard\nK-Means Clustering\nImplement a program that performs k-means clustering on 2D data points. The program should partition data points into k clusters by iteratively assigning points to their nearest centroid and updating centroid positions until convergence. The k-means algorithm works as follows: Initialize k centroids (using the provided initial centroids) Assign each data point to the nearest centroid based on Euclidean distance Recalculate each centroid as the mean of all points assigned to it Repeat steps 2-3 until the centroids move less than a predefined threshold or a maximum number of iterations is reached Implementation Requirements External libraries are not permitted The solve function signature must remain unchanged Algorithm must terminate when either: centroids move less than 0.0001 between iterations, or maximum number of iterations is reached Distance between points must be calculated using Euclidean distance: \\(\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\) Example: Input: data_x = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] data_y = [1.0, 1.5, 1.2, 1.3, 1.1, 5.0, 5.2, 5.1, 5.3, 5.4, 10.1, 10.2, 10.0, 10.3, 10.5] initial_centroid_x = [3.4, 7.1, 8.5] initial_centroid_y = [3.4, 7.1, 8.5] sample_size = 15 k = 3 max_iterations = 20 Output: labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2] final_centroid_x = [1.22, 5.2, 10.22] final_centroid_y = [1.22, 5.2, 10.22] Constraints 1 &le; sample_size &le; 1,000,000 1 &le; k &le; 100 1 &le; max_iterations &le; 1,000 All coordinates are within the range [-10,000, 10,000] Input arrays data_x and data_y have length equal to sample_size Input arrays initial_centroid_x and initial_centroid_y have length equal to k",
      "challenge_url": "https://leetgpu.com/challenges/k-means-clustering",
      "combination_number": 25,
      "framework": "TINYGRAD",
      "gpu": "NVIDIA B200",
      "fastest_time": null,
      "fastest_ms": null,
      "total_timings_found": 0,
      "framework_selected": true,
      "gpu_selected": true,
      "timestamp": "2025-07-07T04:56:04.394326"
    }
  ]
}